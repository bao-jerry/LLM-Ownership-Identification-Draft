# Yes, That's Mine: Asymptotically Foolproof LLM Ownership Verification Against Hidden Adversarial Decoding Parameter Perturbations

## Working Abstract (Draft)
Creating and training an LLM can take vast amounts of resources, money, and effort. Because of this, many LLM creators seek to protect their credit and IP from being stolen. 'LLM ownership verification' is therefore one of the primary commercial motivators of the research of LLM provenance. The constraining practical assumptions needed for white-box methods and the fundamental invasiveness of backdoor-style black-box methods drives the search for reliable, non-invasive black-box methods. We identify a practice-theory gap in the current literature for non-invasive, black-box methods for LLM ownership verification, and we propose a collaborative formal program for classifying threat models which admit 'theoretically foolproof' (in the sense of decidability, statistical consistensy, etc.) methods of LLM ownership verification. As the first step in this formal program, we identify hidden decoding parameter perturbations as the natural first model-identity-obfuscation class to investigate, and we design a group of statistically consistent tests for identifying an LLM's ownership identity under such perturbations. The methods in this paper satisfy several desirable properties such as: 1) being non-invasive and fully black box, 2) being statistically consistent, 3) being viable against an adversary with perfect information, 4) emitting useful intermediate results that may be useful in other research, and 5) being viable for further optimization. Along the way, we will have proven a fundamental negative result: standard, token-distribution-modifying decoding parameters are not enough to prevent an LLM's ownership identity from being verified by a theoretically foolproof method. Furthermore, we will have established the first evidence that the proposed theoretical program for proving similar results under other threat scenarios is viable.
